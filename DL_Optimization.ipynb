{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision tensorflow keras numpy pandas matplotlib"
      ],
      "metadata": {
        "id": "WC-IWdq6FIgL"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.1.0 torchvision"
      ],
      "metadata": {
        "id": "GymroU2N2UKZ"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# Charger un mod√®le pr√©-entra√Æn√©\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.eval()\n",
        "#print(model)"
      ],
      "metadata": {
        "id": "puPTrOAhF4Bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pruning#\n",
        "import torch\n",
        "import torch.nn.utils.prune as prune\n",
        "import torchvision.models as models\n",
        "import os\n",
        "import gzip\n",
        "import shutil\n",
        "import gc\n",
        "\n",
        "# Lib√©rer la m√©moire GPU\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print('Affichage de Pruning :')\n",
        "# Configuration du p√©riph√©rique\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Fonction utilitaire pour obtenir la taille du mod√®le en Ko\n",
        "def get_model_size(model, filename=\"temp_model.pth\"):\n",
        "    torch.save(model.state_dict(), filename)\n",
        "    size_kb = os.path.getsize(filename) / 1024  # Taille en Ko\n",
        "    os.remove(filename)\n",
        "    return size_kb\n",
        "# Fonction pour appliquer le pruning aux couches Conv2d et Linear\n",
        "def apply_pruning(model, amount=0.9):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, (torch.nn.Conv2d, torch.nn.Linear)):\n",
        "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
        "            prune.remove(module, 'weight')  # Supprime le masque apr√®s pruning\n",
        "# Fonction pour compresser un fichier mod√®le avec GZIP\n",
        "def compress_model(input_path, output_path):\n",
        "    with open(input_path, 'rb') as f_in, gzip.open(output_path, 'wb') as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)\n",
        "    return os.path.getsize(output_path) / 1024  # Taille en Ko\n",
        "# Charger le mod√®le pr√©-entra√Æn√©\n",
        "model = models.resnet18(pretrained=True).to(device)\n",
        "model.eval()\n",
        "# Taille avant pruning\n",
        "initial_size = get_model_size(model)\n",
        "print(f\"Taille du mod√®le **avant pruning** : {initial_size:.2f} KB\")\n",
        "# Appliquer le pruning (90% des poids supprim√©s)\n",
        "apply_pruning(model, amount=0.9)\n",
        "# Sauvegarde du mod√®le prun√©\n",
        "pruned_model_path = \"resnet18_pruned.pth\"\n",
        "torch.save(model.state_dict(), pruned_model_path)\n",
        "# Taille apr√®s pruning (toujours dense)\n",
        "pruned_size = get_model_size(model)\n",
        "print(f\"Taille du mod√®le **apr√®s pruning** (dense) : {pruned_size:.2f} KB\")\n",
        "# Compression avec gzip\n",
        "compressed_path = \"resnet18_pruned.pth.gz\"\n",
        "compressed_size = compress_model(pruned_model_path, compressed_path)\n",
        "print(f\"Taille du mod√®le compress√© (.gz) : {compressed_size:.2f} KB\")"
      ],
      "metadata": {
        "id": "zS84mg-G2BQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Quantization#\n",
        "import torch\n",
        "import torch.quantization\n",
        "from torchvision import models\n",
        "print('Affichage de Quantization :')\n",
        "\n",
        "# 1. Charger le mod√®le pr√©-entra√Æn√©\n",
        "model_fp32 = models.resnet18(pretrained=True)\n",
        "model_fp32.eval()\n",
        "\n",
        "# 2. Fusionner les modules (Conv + BN + ReLU)\n",
        "model_fp32_fused = torch.quantization.fuse_modules(\n",
        "    model_fp32,\n",
        "    [['conv1', 'bn1', 'relu']]\n",
        "    + [['layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu'],\n",
        "       ['layer1.0.conv2', 'layer1.0.bn2'],\n",
        "       ['layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu'],\n",
        "       ['layer1.1.conv2', 'layer1.1.bn2']]\n",
        ")\n",
        "\n",
        "# 3. Pr√©parer pour la quantification\n",
        "model_fp32_fused.qconfig = torch.quantization.get_default_qconfig('fbgemm')  # pour x86\n",
        "torch.quantization.prepare(model_fp32_fused, inplace=True)\n",
        "\n",
        "# 5. Convertir en mod√®le quantifi√©\n",
        "model_int8 = torch.quantization.convert(model_fp32_fused, inplace=True)\n",
        "model_int8.eval()\n",
        "\n",
        "# 6. Tester ou sauvegarder\n",
        "torch.save(model_int8.state_dict(), \"resnet18_quantized.pth\")\n",
        "###\n",
        "\n",
        "#comparaison des tailles #\n",
        "import os\n",
        "def get_model_size(model):\n",
        "    torch.save(model.state_dict(), \"temp_model.pth\")\n",
        "    size = os.path.getsize(\"temp_model.pth\") / 1024\n",
        "    os.remove(\"temp_model.pth\")\n",
        "    return size\n",
        "\n",
        "print(\"Taille du mod√®le original:\", get_model_size(model), \"KB\")\n",
        "print(\"Taille du mod√®le quantifi√©:\", get_model_size(model_int8), \"KB\")\n",
        "##\n"
      ],
      "metadata": {
        "id": "IYvRF2L8LM9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Knowledge Distillation#\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "print('Affichage de Knowledge Distillation :')\n",
        "# Configuration\n",
        "device = torch.device(\"cpu\")\n",
        "# Mod√®les Teacher et Student\n",
        "teacher = models.resnet34(pretrained=True).to(device)\n",
        "teacher.eval()\n",
        "student = models.resnet18(pretrained=False).to(device)\n",
        "student.fc = nn.Linear(512, 1000)  # Adapter si n√©cessaire\n",
        "student.train()\n",
        "# Fonction de distillation\n",
        "def distillation_loss(student_logits, teacher_logits, labels, T=10.0, alpha=0.7):\n",
        "    soft_teacher = F.softmax(teacher_logits / T, dim=1)\n",
        "    soft_student = F.log_softmax(student_logits / T, dim=1)\n",
        "    loss_soft = F.kl_div(soft_student, soft_teacher, reduction='batchmean') * (T ** 2)\n",
        "    loss_hard = F.cross_entropy(student_logits, labels)\n",
        "    return alpha * loss_soft + (1 - alpha) * loss_hard\n",
        "# Donn√©es factices (batch de 4 images RGB 224x224)\n",
        "x = torch.randn(4, 3, 224, 224).to(device)\n",
        "y = torch.randint(0, 1000, (4,)).to(device)\n",
        "# Optimiseur\n",
        "optimizer = torch.optim.Adam(student.parameters(), lr=0.001)\n",
        "# Entra√Ænement (5 epochs)\n",
        "for epoch in range(10):\n",
        "    with torch.no_grad():\n",
        "       teacher_logits = teacher(x)\n",
        "\n",
        "    student_logits = student(x)\n",
        "    loss = distillation_loss(student_logits, teacher_logits, y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"üìö Epoch {epoch+1}/10 - Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "aX-kDC5z2Z2u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}